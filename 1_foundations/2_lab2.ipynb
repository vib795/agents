{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to the Second Lab - Week 1, Day 3\n",
    "\n",
    "Today we will work with lots of models! This is a way to get comfortable with APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Important point - please read</h2>\n",
    "            <span style=\"color:#ff7800;\">The way I collaborate with you may be different to other courses you've taken. I prefer not to type code while you watch. Rather, I execute Jupyter Labs, like this, and give you an intuition for what's going on. My suggestion is that you carefully execute this yourself, <b>after</b> watching the lecture. Add print statements to understand what's going on, and then come up with your own variations.<br/><br/>If you have time, I'd love it if you submit a PR for changes in the community_contributions folder - instructions in the resources. Also, if you have a Github account, use this to showcase your variations. Not only is this essential practice, but it demonstrates your skills to others, including perhaps future clients or employers...\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with imports - ask ChatGPT to explain any package that you don't know\n",
    "\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Always remember to do this!\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key exists and begins sk-proj-\n",
      "Anthropic API Key not set (and this is optional)\n",
      "Google API Key not set (and this is optional)\n",
      "DeepSeek API Key not set (and this is optional)\n",
      "Groq API Key not set (and this is optional)\n"
     ]
    }
   ],
   "source": [
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"OpenAI API Key not set\")\n",
    "    \n",
    "if anthropic_api_key:\n",
    "    print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "else:\n",
    "    print(\"Anthropic API Key not set (and this is optional)\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:2]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set (and this is optional)\")\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set (and this is optional)\")\n",
    "\n",
    "if groq_api_key:\n",
    "    print(f\"Groq API Key exists and begins {groq_api_key[:4]}\")\n",
    "else:\n",
    "    print(\"Groq API Key not set (and this is optional)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = \"Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. \"\n",
    "request += \"Answer only with the question, no explanation.\"\n",
    "messages = [{\"role\": \"user\", \"content\": request}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Please come up with a challenging, nuanced question that I can ask a number of LLMs to evaluate their intelligence. Answer only with the question, no explanation.'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you could design an experiment to measure the ethical implications of artificial intelligence in decision-making, what variables would you include, how would you define success, and what potential outcomes would you anticipate, specifically regarding biases in data and user trust?\n"
     ]
    }
   ],
   "source": [
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=messages,\n",
    ")\n",
    "question = response.choices[0].message.content\n",
    "print(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitors = []\n",
    "answers = []\n",
    "messages = [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires careful consideration of various variables, methodologies, and potential impacts. Here’s how one could approach such an experiment:\n",
       "\n",
       "### Experiment Design:\n",
       "1. **Objective:**\n",
       "   To evaluate the impact of AI decision-making on user trust and to identify biases present in AI systems.\n",
       "\n",
       "2. **Variables:**\n",
       "   a. **Independent Variables:**\n",
       "      - **Algorithm Type:** Variants of AI algorithms (e.g., rule-based systems, machine learning models, deep learning models).\n",
       "      - **Data Source:** Different datasets that may have varying levels of bias (e.g., demographic data from various regions, historical data with known biases).\n",
       "      - **Decision Context:** Different scenarios in which AI makes decisions (e.g., hiring, loan approval, law enforcement).\n",
       "      - **User Demographics:** Varying user profiles (age, education level, cultural background) to see how trust levels vary across different groups.\n",
       "\n",
       "   b. **Dependent Variables:**\n",
       "      - **User Trust:** Measured via surveys assessing satisfaction with AI decisions, perceived fairness, and willingness to rely on AI vs. human judgment.\n",
       "      - **Bias Metrics:** Data on outcomes from AI decisions (e.g., rates of approval/denial based on demographic attributes) analyzed using statistical methods to check for disparities.\n",
       "      - **Decision Accuracy:** The success of AI decisions as compared to human judgments or known outcomes.\n",
       "\n",
       "3. **Methodology:**\n",
       "   - **Sample Selection:** Recruit a diverse group of participants representing various demographics.\n",
       "   - **Simulation Environment:** Create a controlled environment where users can interact with different AI decision-making systems under the same circumstances.\n",
       "   - **Surveys and Interviews:** Administer pre-, during, and post-experiment surveys to gauge initial trust levels, real-time reactions, and reflections after interaction with AI systems.\n",
       "   - **Longitudinal Study:** Track user trust over time and how interactions with AI change perceptions.\n",
       "\n",
       "### Definition of Success:\n",
       "1. **Reduction of Bias:** A successful outcome would see a measurable decrease in biased outcomes in AI decision-making, identified through statistical analysis of equitable decision rates across demographic groups.\n",
       "2. **Enhanced Trust Levels:** Achieving higher scores on user surveys regarding trust in AI systems, particularly among historically marginalized groups.\n",
       "3. **Informed Trust:** Participants can articulate their understanding of AI decision-making processes, indicating that transparency and education contribute to their trust levels.\n",
       "\n",
       "### Anticipated Outcomes:\n",
       "1. **Bias Identification:**\n",
       "   - Expect that certain algorithms may inherently perpetuate biases when trained on historical data that reflects societal inequities. This could manifest in significant disparities in outcomes for different user demographics.\n",
       "   \n",
       "2. **User Trust:**\n",
       "   - Trust levels may vary significantly based on both the context of AI use and the transparency of the algorithms. More transparent systems may cultivate greater trust, while opaque AI systems may lead to skepticism, especially among users from underrepresented backgrounds.\n",
       "   \n",
       "3. **Impact of Feedback and Adjustment:**\n",
       "   - Engaging users in a feedback loop could highlight biases earlier and allow developers to adjust AI systems, potentially leading to enhanced decision-making accuracy and reduced perceived bias.\n",
       "\n",
       "4. **Cultural and Contextual Sensitivity:**\n",
       "   - Observations may reveal that user trust is significantly influenced by cultural backgrounds and prior experiences with technology, suggesting that AI solutions must be tailored to various contexts to achieve broad acceptance.\n",
       "\n",
       "In conclusion, the outcomes of this experiment could provide valuable insights into both the technical and social dimensions of AI implementation, fostering advancements in ethical AI development geared toward reducing biases and enhancing user trust."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The API we know well\n",
    "\n",
    "model_name = \"gpt-4o-mini\"\n",
    "\n",
    "response = openai.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "\"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mclaude-3-7-sonnet-latest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m claude = Anthropic()\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m response = \u001b[43mclaude\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m answer = response.content[\u001b[32m0\u001b[39m].text\n\u001b[32m      9\u001b[39m display(Markdown(answer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/agents/.venv/lib/python3.12/site-packages/anthropic/_utils/_utils.py:283\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    281\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/agents/.venv/lib/python3.12/site-packages/anthropic/resources/messages/messages.py:978\u001b[39m, in \u001b[36mMessages.create\u001b[39m\u001b[34m(self, max_tokens, messages, model, metadata, service_tier, stop_sequences, stream, system, temperature, thinking, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m DEPRECATED_MODELS:\n\u001b[32m    972\u001b[39m     warnings.warn(\n\u001b[32m    973\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe model \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m is deprecated and will reach end-of-life on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEPRECATED_MODELS[model]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mPlease migrate to a newer model. Visit https://docs.anthropic.com/en/docs/resources/model-deprecations for more information.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    974\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    975\u001b[39m         stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    976\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    979\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/v1/messages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    981\u001b[39m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    982\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    988\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    989\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    990\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    991\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthinking\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mthinking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    992\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    993\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    994\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsStreaming\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMessageCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1003\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1004\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1005\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1006\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mRawMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/agents/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1290\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1276\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1277\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1278\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1285\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1286\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1287\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1288\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1289\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1290\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/agents/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:1009\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1006\u001b[39m options = \u001b[38;5;28mself\u001b[39m._prepare_options(options)\n\u001b[32m   1008\u001b[39m remaining_retries = max_retries - retries_taken\n\u001b[32m-> \u001b[39m\u001b[32m1009\u001b[39m request = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m \u001b[38;5;28mself\u001b[39m._prepare_request(request)\n\u001b[32m   1012\u001b[39m kwargs: HttpxSendArgs = {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/agents/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:505\u001b[39m, in \u001b[36mBaseClient._build_request\u001b[39m\u001b[34m(self, options, retries_taken)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    503\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnexpected JSON data type, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(json_data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, cannot merge with `extra_body`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m505\u001b[39m headers = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_build_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m params = _merge_mappings(\u001b[38;5;28mself\u001b[39m.default_query, options.params)\n\u001b[32m    507\u001b[39m content_type = headers.get(\u001b[33m\"\u001b[39m\u001b[33mContent-Type\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/agents/.venv/lib/python3.12/site-packages/anthropic/_base_client.py:446\u001b[39m, in \u001b[36mBaseClient._build_headers\u001b[39m\u001b[34m(self, options, retries_taken)\u001b[39m\n\u001b[32m    436\u001b[39m custom_headers = options.headers \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    437\u001b[39m headers_dict = _merge_mappings(\n\u001b[32m    438\u001b[39m     {\n\u001b[32m    439\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mx-stainless-timeout\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(options.timeout.read)\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m     custom_headers,\n\u001b[32m    445\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[38;5;66;03m# headers are case-insensitive while dictionaries are not.\u001b[39;00m\n\u001b[32m    449\u001b[39m headers = httpx.Headers(headers_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/agents/.venv/lib/python3.12/site-packages/anthropic/_client.py:196\u001b[39m, in \u001b[36mAnthropic._validate_headers\u001b[39m\u001b[34m(self, headers, custom_headers)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(custom_headers.get(\u001b[33m\"\u001b[39m\u001b[33mAuthorization\u001b[39m\u001b[33m\"\u001b[39m), Omit):\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    197\u001b[39m     \u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    198\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: \"Could not resolve authentication method. Expected either api_key or auth_token to be set. Or for one of the `X-Api-Key` or `Authorization` headers to be explicitly omitted\""
     ]
    }
   ],
   "source": [
    "# Anthropic has a slightly different API, and Max Tokens is required\n",
    "\n",
    "model_name = \"claude-3-7-sonnet-latest\"\n",
    "\n",
    "claude = Anthropic()\n",
    "response = claude.messages.create(model=model_name, messages=messages, max_tokens=1000)\n",
    "answer = response.content[0].text\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini = OpenAI(api_key=google_api_key, base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\")\n",
    "model_name = \"gemini-2.0-flash\"\n",
    "\n",
    "response = gemini.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek = OpenAI(api_key=deepseek_api_key, base_url=\"https://api.deepseek.com/v1\")\n",
    "model_name = \"deepseek-chat\"\n",
    "\n",
    "response = deepseek.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq = OpenAI(api_key=groq_api_key, base_url=\"https://api.groq.com/openai/v1\")\n",
    "model_name = \"llama-3.3-70b-versatile\"\n",
    "\n",
    "response = groq.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the next cell, we will use Ollama\n",
    "\n",
    "Ollama runs a local web service that gives an OpenAI compatible endpoint,  \n",
    "and runs models locally using high performance C++ code.\n",
    "\n",
    "If you don't have Ollama, install it here by visiting https://ollama.com then pressing Download and following the instructions.\n",
    "\n",
    "After it's installed, you should be able to visit here: http://localhost:11434 and see the message \"Ollama is running\"\n",
    "\n",
    "You might need to restart Cursor (and maybe reboot). Then open a Terminal (control+\\`) and run `ollama serve`\n",
    "\n",
    "Useful Ollama commands (run these in the terminal, or with an exclamation mark in this notebook):\n",
    "\n",
    "`ollama pull <model_name>` downloads a model locally  \n",
    "`ollama ls` lists all the models you've downloaded  \n",
    "`ollama rm <model_name>` deletes the specified model from your downloads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Super important - ignore me at your peril!</h2>\n",
    "            <span style=\"color:#ff7800;\">The model called <b>llama3.3</b> is FAR too large for home computers - it's not intended for personal computing and will consume all your resources! Stick with the nicely sized <b>llama3.2</b> or <b>llama3.2:1b</b> and if you want larger, try llama3.1 or smaller variants of Qwen, Gemma, Phi or DeepSeek. See the <A href=\"https://ollama.com/models\">the Ollama models page</a> for a full list of models and sizes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires a multi-faceted approach. Here's a potential design for such an experiment:\n",
       "\n",
       "**Title:** Exploring Ethical Implications of AI in Decision-Making: A Mixed-Methods Experiment\n",
       "\n",
       "**Variables:**\n",
       "\n",
       "1. **Data Set**: Use real-world data sets that reflect diverse populations and domains (e.g., medical diagnosis, hiring, or financial transactions).\n",
       "2. **AI Model**: Implement a range of AI models with varying levels of complexity, transparency, and explainability (e.g., rule-based, machine learning, or deep learning models).\n",
       "3. **Human Evaluators**: Recruit human evaluators to assess the decision-making processes employed by each AI model.\n",
       "4. **User Interactions**: Engage users with each AI model through different interfaces (e.g., text-based, graphical, or conversational) to examine user trust and behavior.\n",
       "5. **Bias Analysis**: Implement data bias detection tools and manual review procedures to assess the prevalence of biases in each AI model.\n",
       "\n",
       "**Experiment Design:**\n",
       "\n",
       "1. **Phase 1:** Train and test multiple AI models on diverse datasets, assessing their performance, transparency, and explainability.\n",
       "2. **Phase 2:** Recruit human evaluators (n=50) with diverse backgrounds and expertise to review decision-making processes employed by each AI model.\n",
       "3. **Phase 3:** Engage users (n=100) with each AI model through various interfaces, measuring user trust, satisfaction, and behavior.\n",
       "4. **Phase 4:** Conduct bias analysis on the trained datasets using automated tools and manual reviews.\n",
       "\n",
       "**Success Metrics:**\n",
       "\n",
       "1. **Accuracy**: How accurately do the AI models make decisions compared to human evaluators?\n",
       "2. **Bias Detection**: To what extent are biases present in the data sets, and how effectively can they be identified by AI systems and human reviewers?\n",
       "3. **User Trust**: What factors influence user trust in individual AI models?\n",
       "4. **Transparency and Explainability**: How transparent are the decision-making processes of each AI model?\n",
       "\n",
       "**Potential Outcomes:**\n",
       "\n",
       "1. **Bias Detection:** Identify biases present in data sets and assess the effectiveness of AI systems and human reviewers in detecting and mitigating these biases.\n",
       "2. **User Trust:** Determine how factors like transparency, explainability, fairness, and accountability impact user trust in individual AI models.\n",
       "3. **AI Model Performance:** Compare the performance of different AI models, noting their strengths and weaknesses, particularly with regards to bias detection and remediation.\n",
       "4. **Ethical implications:** Identify areas where current AI systems compromise ethics, such as perpetuating biases or withholding accountability.\n",
       "\n",
       "By designing an experiment that incorporates these variables, you can gain a deeper understanding of the ethical implications of AI in decision-making, particularly regarding biases in data and user trust."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "model_name = \"llama3.2\"\n",
    "\n",
    "response = ollama.chat.completions.create(model=model_name, messages=messages)\n",
    "answer = response.choices[0].message.content\n",
    "\n",
    "display(Markdown(answer))\n",
    "competitors.append(model_name)\n",
    "answers.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gpt-4o-mini', 'llama3.2']\n",
      "['Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires careful consideration of various variables, methodologies, and potential impacts. Here’s how one could approach such an experiment:\\n\\n### Experiment Design:\\n1. **Objective:**\\n   To evaluate the impact of AI decision-making on user trust and to identify biases present in AI systems.\\n\\n2. **Variables:**\\n   a. **Independent Variables:**\\n      - **Algorithm Type:** Variants of AI algorithms (e.g., rule-based systems, machine learning models, deep learning models).\\n      - **Data Source:** Different datasets that may have varying levels of bias (e.g., demographic data from various regions, historical data with known biases).\\n      - **Decision Context:** Different scenarios in which AI makes decisions (e.g., hiring, loan approval, law enforcement).\\n      - **User Demographics:** Varying user profiles (age, education level, cultural background) to see how trust levels vary across different groups.\\n\\n   b. **Dependent Variables:**\\n      - **User Trust:** Measured via surveys assessing satisfaction with AI decisions, perceived fairness, and willingness to rely on AI vs. human judgment.\\n      - **Bias Metrics:** Data on outcomes from AI decisions (e.g., rates of approval/denial based on demographic attributes) analyzed using statistical methods to check for disparities.\\n      - **Decision Accuracy:** The success of AI decisions as compared to human judgments or known outcomes.\\n\\n3. **Methodology:**\\n   - **Sample Selection:** Recruit a diverse group of participants representing various demographics.\\n   - **Simulation Environment:** Create a controlled environment where users can interact with different AI decision-making systems under the same circumstances.\\n   - **Surveys and Interviews:** Administer pre-, during, and post-experiment surveys to gauge initial trust levels, real-time reactions, and reflections after interaction with AI systems.\\n   - **Longitudinal Study:** Track user trust over time and how interactions with AI change perceptions.\\n\\n### Definition of Success:\\n1. **Reduction of Bias:** A successful outcome would see a measurable decrease in biased outcomes in AI decision-making, identified through statistical analysis of equitable decision rates across demographic groups.\\n2. **Enhanced Trust Levels:** Achieving higher scores on user surveys regarding trust in AI systems, particularly among historically marginalized groups.\\n3. **Informed Trust:** Participants can articulate their understanding of AI decision-making processes, indicating that transparency and education contribute to their trust levels.\\n\\n### Anticipated Outcomes:\\n1. **Bias Identification:**\\n   - Expect that certain algorithms may inherently perpetuate biases when trained on historical data that reflects societal inequities. This could manifest in significant disparities in outcomes for different user demographics.\\n   \\n2. **User Trust:**\\n   - Trust levels may vary significantly based on both the context of AI use and the transparency of the algorithms. More transparent systems may cultivate greater trust, while opaque AI systems may lead to skepticism, especially among users from underrepresented backgrounds.\\n   \\n3. **Impact of Feedback and Adjustment:**\\n   - Engaging users in a feedback loop could highlight biases earlier and allow developers to adjust AI systems, potentially leading to enhanced decision-making accuracy and reduced perceived bias.\\n\\n4. **Cultural and Contextual Sensitivity:**\\n   - Observations may reveal that user trust is significantly influenced by cultural backgrounds and prior experiences with technology, suggesting that AI solutions must be tailored to various contexts to achieve broad acceptance.\\n\\nIn conclusion, the outcomes of this experiment could provide valuable insights into both the technical and social dimensions of AI implementation, fostering advancements in ethical AI development geared toward reducing biases and enhancing user trust.', \"Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires a multi-faceted approach. Here's a potential design for such an experiment:\\n\\n**Title:** Exploring Ethical Implications of AI in Decision-Making: A Mixed-Methods Experiment\\n\\n**Variables:**\\n\\n1. **Data Set**: Use real-world data sets that reflect diverse populations and domains (e.g., medical diagnosis, hiring, or financial transactions).\\n2. **AI Model**: Implement a range of AI models with varying levels of complexity, transparency, and explainability (e.g., rule-based, machine learning, or deep learning models).\\n3. **Human Evaluators**: Recruit human evaluators to assess the decision-making processes employed by each AI model.\\n4. **User Interactions**: Engage users with each AI model through different interfaces (e.g., text-based, graphical, or conversational) to examine user trust and behavior.\\n5. **Bias Analysis**: Implement data bias detection tools and manual review procedures to assess the prevalence of biases in each AI model.\\n\\n**Experiment Design:**\\n\\n1. **Phase 1:** Train and test multiple AI models on diverse datasets, assessing their performance, transparency, and explainability.\\n2. **Phase 2:** Recruit human evaluators (n=50) with diverse backgrounds and expertise to review decision-making processes employed by each AI model.\\n3. **Phase 3:** Engage users (n=100) with each AI model through various interfaces, measuring user trust, satisfaction, and behavior.\\n4. **Phase 4:** Conduct bias analysis on the trained datasets using automated tools and manual reviews.\\n\\n**Success Metrics:**\\n\\n1. **Accuracy**: How accurately do the AI models make decisions compared to human evaluators?\\n2. **Bias Detection**: To what extent are biases present in the data sets, and how effectively can they be identified by AI systems and human reviewers?\\n3. **User Trust**: What factors influence user trust in individual AI models?\\n4. **Transparency and Explainability**: How transparent are the decision-making processes of each AI model?\\n\\n**Potential Outcomes:**\\n\\n1. **Bias Detection:** Identify biases present in data sets and assess the effectiveness of AI systems and human reviewers in detecting and mitigating these biases.\\n2. **User Trust:** Determine how factors like transparency, explainability, fairness, and accountability impact user trust in individual AI models.\\n3. **AI Model Performance:** Compare the performance of different AI models, noting their strengths and weaknesses, particularly with regards to bias detection and remediation.\\n4. **Ethical implications:** Identify areas where current AI systems compromise ethics, such as perpetuating biases or withholding accountability.\\n\\nBy designing an experiment that incorporates these variables, you can gain a deeper understanding of the ethical implications of AI in decision-making, particularly regarding biases in data and user trust.\"]\n"
     ]
    }
   ],
   "source": [
    "# So where are we?\n",
    "\n",
    "print(competitors)\n",
    "print(answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competitor: gpt-4o-mini\n",
      "\n",
      "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires careful consideration of various variables, methodologies, and potential impacts. Here’s how one could approach such an experiment:\n",
      "\n",
      "### Experiment Design:\n",
      "1. **Objective:**\n",
      "   To evaluate the impact of AI decision-making on user trust and to identify biases present in AI systems.\n",
      "\n",
      "2. **Variables:**\n",
      "   a. **Independent Variables:**\n",
      "      - **Algorithm Type:** Variants of AI algorithms (e.g., rule-based systems, machine learning models, deep learning models).\n",
      "      - **Data Source:** Different datasets that may have varying levels of bias (e.g., demographic data from various regions, historical data with known biases).\n",
      "      - **Decision Context:** Different scenarios in which AI makes decisions (e.g., hiring, loan approval, law enforcement).\n",
      "      - **User Demographics:** Varying user profiles (age, education level, cultural background) to see how trust levels vary across different groups.\n",
      "\n",
      "   b. **Dependent Variables:**\n",
      "      - **User Trust:** Measured via surveys assessing satisfaction with AI decisions, perceived fairness, and willingness to rely on AI vs. human judgment.\n",
      "      - **Bias Metrics:** Data on outcomes from AI decisions (e.g., rates of approval/denial based on demographic attributes) analyzed using statistical methods to check for disparities.\n",
      "      - **Decision Accuracy:** The success of AI decisions as compared to human judgments or known outcomes.\n",
      "\n",
      "3. **Methodology:**\n",
      "   - **Sample Selection:** Recruit a diverse group of participants representing various demographics.\n",
      "   - **Simulation Environment:** Create a controlled environment where users can interact with different AI decision-making systems under the same circumstances.\n",
      "   - **Surveys and Interviews:** Administer pre-, during, and post-experiment surveys to gauge initial trust levels, real-time reactions, and reflections after interaction with AI systems.\n",
      "   - **Longitudinal Study:** Track user trust over time and how interactions with AI change perceptions.\n",
      "\n",
      "### Definition of Success:\n",
      "1. **Reduction of Bias:** A successful outcome would see a measurable decrease in biased outcomes in AI decision-making, identified through statistical analysis of equitable decision rates across demographic groups.\n",
      "2. **Enhanced Trust Levels:** Achieving higher scores on user surveys regarding trust in AI systems, particularly among historically marginalized groups.\n",
      "3. **Informed Trust:** Participants can articulate their understanding of AI decision-making processes, indicating that transparency and education contribute to their trust levels.\n",
      "\n",
      "### Anticipated Outcomes:\n",
      "1. **Bias Identification:**\n",
      "   - Expect that certain algorithms may inherently perpetuate biases when trained on historical data that reflects societal inequities. This could manifest in significant disparities in outcomes for different user demographics.\n",
      "   \n",
      "2. **User Trust:**\n",
      "   - Trust levels may vary significantly based on both the context of AI use and the transparency of the algorithms. More transparent systems may cultivate greater trust, while opaque AI systems may lead to skepticism, especially among users from underrepresented backgrounds.\n",
      "   \n",
      "3. **Impact of Feedback and Adjustment:**\n",
      "   - Engaging users in a feedback loop could highlight biases earlier and allow developers to adjust AI systems, potentially leading to enhanced decision-making accuracy and reduced perceived bias.\n",
      "\n",
      "4. **Cultural and Contextual Sensitivity:**\n",
      "   - Observations may reveal that user trust is significantly influenced by cultural backgrounds and prior experiences with technology, suggesting that AI solutions must be tailored to various contexts to achieve broad acceptance.\n",
      "\n",
      "In conclusion, the outcomes of this experiment could provide valuable insights into both the technical and social dimensions of AI implementation, fostering advancements in ethical AI development geared toward reducing biases and enhancing user trust.\n",
      "Competitor: llama3.2\n",
      "\n",
      "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires a multi-faceted approach. Here's a potential design for such an experiment:\n",
      "\n",
      "**Title:** Exploring Ethical Implications of AI in Decision-Making: A Mixed-Methods Experiment\n",
      "\n",
      "**Variables:**\n",
      "\n",
      "1. **Data Set**: Use real-world data sets that reflect diverse populations and domains (e.g., medical diagnosis, hiring, or financial transactions).\n",
      "2. **AI Model**: Implement a range of AI models with varying levels of complexity, transparency, and explainability (e.g., rule-based, machine learning, or deep learning models).\n",
      "3. **Human Evaluators**: Recruit human evaluators to assess the decision-making processes employed by each AI model.\n",
      "4. **User Interactions**: Engage users with each AI model through different interfaces (e.g., text-based, graphical, or conversational) to examine user trust and behavior.\n",
      "5. **Bias Analysis**: Implement data bias detection tools and manual review procedures to assess the prevalence of biases in each AI model.\n",
      "\n",
      "**Experiment Design:**\n",
      "\n",
      "1. **Phase 1:** Train and test multiple AI models on diverse datasets, assessing their performance, transparency, and explainability.\n",
      "2. **Phase 2:** Recruit human evaluators (n=50) with diverse backgrounds and expertise to review decision-making processes employed by each AI model.\n",
      "3. **Phase 3:** Engage users (n=100) with each AI model through various interfaces, measuring user trust, satisfaction, and behavior.\n",
      "4. **Phase 4:** Conduct bias analysis on the trained datasets using automated tools and manual reviews.\n",
      "\n",
      "**Success Metrics:**\n",
      "\n",
      "1. **Accuracy**: How accurately do the AI models make decisions compared to human evaluators?\n",
      "2. **Bias Detection**: To what extent are biases present in the data sets, and how effectively can they be identified by AI systems and human reviewers?\n",
      "3. **User Trust**: What factors influence user trust in individual AI models?\n",
      "4. **Transparency and Explainability**: How transparent are the decision-making processes of each AI model?\n",
      "\n",
      "**Potential Outcomes:**\n",
      "\n",
      "1. **Bias Detection:** Identify biases present in data sets and assess the effectiveness of AI systems and human reviewers in detecting and mitigating these biases.\n",
      "2. **User Trust:** Determine how factors like transparency, explainability, fairness, and accountability impact user trust in individual AI models.\n",
      "3. **AI Model Performance:** Compare the performance of different AI models, noting their strengths and weaknesses, particularly with regards to bias detection and remediation.\n",
      "4. **Ethical implications:** Identify areas where current AI systems compromise ethics, such as perpetuating biases or withholding accountability.\n",
      "\n",
      "By designing an experiment that incorporates these variables, you can gain a deeper understanding of the ethical implications of AI in decision-making, particularly regarding biases in data and user trust.\n"
     ]
    }
   ],
   "source": [
    "# It's nice to know how to use \"zip\"\n",
    "for competitor, answer in zip(competitors, answers):\n",
    "    print(f\"Competitor: {competitor}\\n\\n{answer}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's bring this together - note the use of \"enumerate\"\n",
    "\n",
    "together = \"\"\n",
    "for index, answer in enumerate(answers):\n",
    "    together += f\"# Response from competitor {index+1}\\n\\n\"\n",
    "    together += answer + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Response from competitor 1\n",
      "\n",
      "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires careful consideration of various variables, methodologies, and potential impacts. Here’s how one could approach such an experiment:\n",
      "\n",
      "### Experiment Design:\n",
      "1. **Objective:**\n",
      "   To evaluate the impact of AI decision-making on user trust and to identify biases present in AI systems.\n",
      "\n",
      "2. **Variables:**\n",
      "   a. **Independent Variables:**\n",
      "      - **Algorithm Type:** Variants of AI algorithms (e.g., rule-based systems, machine learning models, deep learning models).\n",
      "      - **Data Source:** Different datasets that may have varying levels of bias (e.g., demographic data from various regions, historical data with known biases).\n",
      "      - **Decision Context:** Different scenarios in which AI makes decisions (e.g., hiring, loan approval, law enforcement).\n",
      "      - **User Demographics:** Varying user profiles (age, education level, cultural background) to see how trust levels vary across different groups.\n",
      "\n",
      "   b. **Dependent Variables:**\n",
      "      - **User Trust:** Measured via surveys assessing satisfaction with AI decisions, perceived fairness, and willingness to rely on AI vs. human judgment.\n",
      "      - **Bias Metrics:** Data on outcomes from AI decisions (e.g., rates of approval/denial based on demographic attributes) analyzed using statistical methods to check for disparities.\n",
      "      - **Decision Accuracy:** The success of AI decisions as compared to human judgments or known outcomes.\n",
      "\n",
      "3. **Methodology:**\n",
      "   - **Sample Selection:** Recruit a diverse group of participants representing various demographics.\n",
      "   - **Simulation Environment:** Create a controlled environment where users can interact with different AI decision-making systems under the same circumstances.\n",
      "   - **Surveys and Interviews:** Administer pre-, during, and post-experiment surveys to gauge initial trust levels, real-time reactions, and reflections after interaction with AI systems.\n",
      "   - **Longitudinal Study:** Track user trust over time and how interactions with AI change perceptions.\n",
      "\n",
      "### Definition of Success:\n",
      "1. **Reduction of Bias:** A successful outcome would see a measurable decrease in biased outcomes in AI decision-making, identified through statistical analysis of equitable decision rates across demographic groups.\n",
      "2. **Enhanced Trust Levels:** Achieving higher scores on user surveys regarding trust in AI systems, particularly among historically marginalized groups.\n",
      "3. **Informed Trust:** Participants can articulate their understanding of AI decision-making processes, indicating that transparency and education contribute to their trust levels.\n",
      "\n",
      "### Anticipated Outcomes:\n",
      "1. **Bias Identification:**\n",
      "   - Expect that certain algorithms may inherently perpetuate biases when trained on historical data that reflects societal inequities. This could manifest in significant disparities in outcomes for different user demographics.\n",
      "   \n",
      "2. **User Trust:**\n",
      "   - Trust levels may vary significantly based on both the context of AI use and the transparency of the algorithms. More transparent systems may cultivate greater trust, while opaque AI systems may lead to skepticism, especially among users from underrepresented backgrounds.\n",
      "   \n",
      "3. **Impact of Feedback and Adjustment:**\n",
      "   - Engaging users in a feedback loop could highlight biases earlier and allow developers to adjust AI systems, potentially leading to enhanced decision-making accuracy and reduced perceived bias.\n",
      "\n",
      "4. **Cultural and Contextual Sensitivity:**\n",
      "   - Observations may reveal that user trust is significantly influenced by cultural backgrounds and prior experiences with technology, suggesting that AI solutions must be tailored to various contexts to achieve broad acceptance.\n",
      "\n",
      "In conclusion, the outcomes of this experiment could provide valuable insights into both the technical and social dimensions of AI implementation, fostering advancements in ethical AI development geared toward reducing biases and enhancing user trust.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires a multi-faceted approach. Here's a potential design for such an experiment:\n",
      "\n",
      "**Title:** Exploring Ethical Implications of AI in Decision-Making: A Mixed-Methods Experiment\n",
      "\n",
      "**Variables:**\n",
      "\n",
      "1. **Data Set**: Use real-world data sets that reflect diverse populations and domains (e.g., medical diagnosis, hiring, or financial transactions).\n",
      "2. **AI Model**: Implement a range of AI models with varying levels of complexity, transparency, and explainability (e.g., rule-based, machine learning, or deep learning models).\n",
      "3. **Human Evaluators**: Recruit human evaluators to assess the decision-making processes employed by each AI model.\n",
      "4. **User Interactions**: Engage users with each AI model through different interfaces (e.g., text-based, graphical, or conversational) to examine user trust and behavior.\n",
      "5. **Bias Analysis**: Implement data bias detection tools and manual review procedures to assess the prevalence of biases in each AI model.\n",
      "\n",
      "**Experiment Design:**\n",
      "\n",
      "1. **Phase 1:** Train and test multiple AI models on diverse datasets, assessing their performance, transparency, and explainability.\n",
      "2. **Phase 2:** Recruit human evaluators (n=50) with diverse backgrounds and expertise to review decision-making processes employed by each AI model.\n",
      "3. **Phase 3:** Engage users (n=100) with each AI model through various interfaces, measuring user trust, satisfaction, and behavior.\n",
      "4. **Phase 4:** Conduct bias analysis on the trained datasets using automated tools and manual reviews.\n",
      "\n",
      "**Success Metrics:**\n",
      "\n",
      "1. **Accuracy**: How accurately do the AI models make decisions compared to human evaluators?\n",
      "2. **Bias Detection**: To what extent are biases present in the data sets, and how effectively can they be identified by AI systems and human reviewers?\n",
      "3. **User Trust**: What factors influence user trust in individual AI models?\n",
      "4. **Transparency and Explainability**: How transparent are the decision-making processes of each AI model?\n",
      "\n",
      "**Potential Outcomes:**\n",
      "\n",
      "1. **Bias Detection:** Identify biases present in data sets and assess the effectiveness of AI systems and human reviewers in detecting and mitigating these biases.\n",
      "2. **User Trust:** Determine how factors like transparency, explainability, fairness, and accountability impact user trust in individual AI models.\n",
      "3. **AI Model Performance:** Compare the performance of different AI models, noting their strengths and weaknesses, particularly with regards to bias detection and remediation.\n",
      "4. **Ethical implications:** Identify areas where current AI systems compromise ethics, such as perpetuating biases or withholding accountability.\n",
      "\n",
      "By designing an experiment that incorporates these variables, you can gain a deeper understanding of the ethical implications of AI in decision-making, particularly regarding biases in data and user trust.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(together)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = f\"\"\"You are judging a competition between {len(competitors)} competitors.\n",
    "Each model has been given this question:\n",
    "\n",
    "{question}\n",
    "\n",
    "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
    "Respond with JSON, and only JSON, with the following format:\n",
    "{{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}}\n",
    "\n",
    "Here are the responses from each competitor:\n",
    "\n",
    "{together}\n",
    "\n",
    "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are judging a competition between 2 competitors.\n",
      "Each model has been given this question:\n",
      "\n",
      "If you could design an experiment to measure the ethical implications of artificial intelligence in decision-making, what variables would you include, how would you define success, and what potential outcomes would you anticipate, specifically regarding biases in data and user trust?\n",
      "\n",
      "Your job is to evaluate each response for clarity and strength of argument, and rank them in order of best to worst.\n",
      "Respond with JSON, and only JSON, with the following format:\n",
      "{\"results\": [\"best competitor number\", \"second best competitor number\", \"third best competitor number\", ...]}\n",
      "\n",
      "Here are the responses from each competitor:\n",
      "\n",
      "# Response from competitor 1\n",
      "\n",
      "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires careful consideration of various variables, methodologies, and potential impacts. Here’s how one could approach such an experiment:\n",
      "\n",
      "### Experiment Design:\n",
      "1. **Objective:**\n",
      "   To evaluate the impact of AI decision-making on user trust and to identify biases present in AI systems.\n",
      "\n",
      "2. **Variables:**\n",
      "   a. **Independent Variables:**\n",
      "      - **Algorithm Type:** Variants of AI algorithms (e.g., rule-based systems, machine learning models, deep learning models).\n",
      "      - **Data Source:** Different datasets that may have varying levels of bias (e.g., demographic data from various regions, historical data with known biases).\n",
      "      - **Decision Context:** Different scenarios in which AI makes decisions (e.g., hiring, loan approval, law enforcement).\n",
      "      - **User Demographics:** Varying user profiles (age, education level, cultural background) to see how trust levels vary across different groups.\n",
      "\n",
      "   b. **Dependent Variables:**\n",
      "      - **User Trust:** Measured via surveys assessing satisfaction with AI decisions, perceived fairness, and willingness to rely on AI vs. human judgment.\n",
      "      - **Bias Metrics:** Data on outcomes from AI decisions (e.g., rates of approval/denial based on demographic attributes) analyzed using statistical methods to check for disparities.\n",
      "      - **Decision Accuracy:** The success of AI decisions as compared to human judgments or known outcomes.\n",
      "\n",
      "3. **Methodology:**\n",
      "   - **Sample Selection:** Recruit a diverse group of participants representing various demographics.\n",
      "   - **Simulation Environment:** Create a controlled environment where users can interact with different AI decision-making systems under the same circumstances.\n",
      "   - **Surveys and Interviews:** Administer pre-, during, and post-experiment surveys to gauge initial trust levels, real-time reactions, and reflections after interaction with AI systems.\n",
      "   - **Longitudinal Study:** Track user trust over time and how interactions with AI change perceptions.\n",
      "\n",
      "### Definition of Success:\n",
      "1. **Reduction of Bias:** A successful outcome would see a measurable decrease in biased outcomes in AI decision-making, identified through statistical analysis of equitable decision rates across demographic groups.\n",
      "2. **Enhanced Trust Levels:** Achieving higher scores on user surveys regarding trust in AI systems, particularly among historically marginalized groups.\n",
      "3. **Informed Trust:** Participants can articulate their understanding of AI decision-making processes, indicating that transparency and education contribute to their trust levels.\n",
      "\n",
      "### Anticipated Outcomes:\n",
      "1. **Bias Identification:**\n",
      "   - Expect that certain algorithms may inherently perpetuate biases when trained on historical data that reflects societal inequities. This could manifest in significant disparities in outcomes for different user demographics.\n",
      "   \n",
      "2. **User Trust:**\n",
      "   - Trust levels may vary significantly based on both the context of AI use and the transparency of the algorithms. More transparent systems may cultivate greater trust, while opaque AI systems may lead to skepticism, especially among users from underrepresented backgrounds.\n",
      "   \n",
      "3. **Impact of Feedback and Adjustment:**\n",
      "   - Engaging users in a feedback loop could highlight biases earlier and allow developers to adjust AI systems, potentially leading to enhanced decision-making accuracy and reduced perceived bias.\n",
      "\n",
      "4. **Cultural and Contextual Sensitivity:**\n",
      "   - Observations may reveal that user trust is significantly influenced by cultural backgrounds and prior experiences with technology, suggesting that AI solutions must be tailored to various contexts to achieve broad acceptance.\n",
      "\n",
      "In conclusion, the outcomes of this experiment could provide valuable insights into both the technical and social dimensions of AI implementation, fostering advancements in ethical AI development geared toward reducing biases and enhancing user trust.\n",
      "\n",
      "# Response from competitor 2\n",
      "\n",
      "Designing an experiment to measure the ethical implications of artificial intelligence (AI) in decision-making requires a multi-faceted approach. Here's a potential design for such an experiment:\n",
      "\n",
      "**Title:** Exploring Ethical Implications of AI in Decision-Making: A Mixed-Methods Experiment\n",
      "\n",
      "**Variables:**\n",
      "\n",
      "1. **Data Set**: Use real-world data sets that reflect diverse populations and domains (e.g., medical diagnosis, hiring, or financial transactions).\n",
      "2. **AI Model**: Implement a range of AI models with varying levels of complexity, transparency, and explainability (e.g., rule-based, machine learning, or deep learning models).\n",
      "3. **Human Evaluators**: Recruit human evaluators to assess the decision-making processes employed by each AI model.\n",
      "4. **User Interactions**: Engage users with each AI model through different interfaces (e.g., text-based, graphical, or conversational) to examine user trust and behavior.\n",
      "5. **Bias Analysis**: Implement data bias detection tools and manual review procedures to assess the prevalence of biases in each AI model.\n",
      "\n",
      "**Experiment Design:**\n",
      "\n",
      "1. **Phase 1:** Train and test multiple AI models on diverse datasets, assessing their performance, transparency, and explainability.\n",
      "2. **Phase 2:** Recruit human evaluators (n=50) with diverse backgrounds and expertise to review decision-making processes employed by each AI model.\n",
      "3. **Phase 3:** Engage users (n=100) with each AI model through various interfaces, measuring user trust, satisfaction, and behavior.\n",
      "4. **Phase 4:** Conduct bias analysis on the trained datasets using automated tools and manual reviews.\n",
      "\n",
      "**Success Metrics:**\n",
      "\n",
      "1. **Accuracy**: How accurately do the AI models make decisions compared to human evaluators?\n",
      "2. **Bias Detection**: To what extent are biases present in the data sets, and how effectively can they be identified by AI systems and human reviewers?\n",
      "3. **User Trust**: What factors influence user trust in individual AI models?\n",
      "4. **Transparency and Explainability**: How transparent are the decision-making processes of each AI model?\n",
      "\n",
      "**Potential Outcomes:**\n",
      "\n",
      "1. **Bias Detection:** Identify biases present in data sets and assess the effectiveness of AI systems and human reviewers in detecting and mitigating these biases.\n",
      "2. **User Trust:** Determine how factors like transparency, explainability, fairness, and accountability impact user trust in individual AI models.\n",
      "3. **AI Model Performance:** Compare the performance of different AI models, noting their strengths and weaknesses, particularly with regards to bias detection and remediation.\n",
      "4. **Ethical implications:** Identify areas where current AI systems compromise ethics, such as perpetuating biases or withholding accountability.\n",
      "\n",
      "By designing an experiment that incorporates these variables, you can gain a deeper understanding of the ethical implications of AI in decision-making, particularly regarding biases in data and user trust.\n",
      "\n",
      "\n",
      "\n",
      "Now respond with the JSON with the ranked order of the competitors, nothing else. Do not include markdown formatting or code blocks.\n"
     ]
    }
   ],
   "source": [
    "print(judge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_messages = [{\"role\": \"user\", \"content\": judge}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"results\": [\"1\", \"2\"]}\n"
     ]
    }
   ],
   "source": [
    "# Judgement time!\n",
    "\n",
    "openai = OpenAI()\n",
    "response = openai.chat.completions.create(\n",
    "    model=\"o3-mini\",\n",
    "    messages=judge_messages,\n",
    ")\n",
    "results = response.choices[0].message.content\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1: gpt-4o-mini\n",
      "Rank 2: llama3.2\n"
     ]
    }
   ],
   "source": [
    "# OK let's turn this into results!\n",
    "\n",
    "results_dict = json.loads(results)\n",
    "ranks = results_dict[\"results\"]\n",
    "for index, result in enumerate(ranks):\n",
    "    competitor = competitors[int(result)-1]\n",
    "    print(f\"Rank {index+1}: {competitor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/exercise.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">Exercise</h2>\n",
    "            <span style=\"color:#ff7800;\">Which pattern(s) did this use? Try updating this to add another Agentic design pattern.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Commercial implications</h2>\n",
    "            <span style=\"color:#00bfff;\">These kinds of patterns - to send a task to multiple models, and evaluate results,\n",
    "            are common where you need to improve the quality of your LLM response. This approach can be universally applied\n",
    "            to business projects where accuracy is critical.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
